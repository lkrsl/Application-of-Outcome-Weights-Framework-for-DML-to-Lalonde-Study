[["index.html", "Application of Outcome Weights Framework for DML to Lalonde Study Preface 0.1 Overview", " Application of Outcome Weights Framework for DML to Lalonde Study Laura Kreisel 2025-10-03 Preface This book demonstrates the application of the Outcome Weights Framework for Double Machine Learning to the Lalonde Study. The framework provides a systematic approach to causal inference in observational studies. 0.1 Overview This tutorial covers the implementation and application of the Outcome Weights Framework to the classic Lalonde dataset for evaluating the effect of job training programs. "],["getting-started.html", "Chapter 1 Getting started 1.1 Installation 1.2 Wrapper functions 1.3 Outcome weights analysis 1.4 Sensitivity analysis", " Chapter 1 Getting started The functions presented below constitute the core wrapper functions utilized throughout the analysis, including three adapted function originally from the Imbens &amp; Xu (2024) tutorial, which were modified to suit specific analytical purpose (highlighted by ***). For the full original collection of functions by Imbens &amp; Xu (2024), including additional functions used in the analysis, please consult their official source code available on their GitHub repository. 1.1 Installation Several R packages are required for the data analysis and visualization. The code below checks for all required packages and installs those that are missing. After the installation, the packages need to be loaded. Since the analysis sources functions directly from the Imbens &amp; Xu (2024) tutorial, the required packages are automatically installed and loaded as part of that process. Therefore, it is only necessary to install and load any additional packages as well as add custom functions that are required for the analysis. Packages: “data.table”, “dplyr”, “ggplot2”, “gridExtra”, “highr”, “highs” , “MatchIt”, “optmatch”, “optweight”, “quickmatch”, “readr”, “rgenoud”, “tidyr”, “tidyverse”, “WeightIt” # required packages packages &lt;- c(&quot;data.table&quot;, &quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;gridExtra&quot;, &quot;highr&quot;, &quot;highs&quot; , &quot;MatchIt&quot;, &quot;optmatch&quot;, &quot;optweight&quot;, &quot;quickmatch&quot;, &quot;readr&quot;, &quot;rgenoud&quot;, &quot;tidyr&quot;, &quot;tidyverse&quot;, &quot;WeightIt&quot; ) # install packages install_all &lt;- function(packages) { installed_pkgs &lt;- installed.packages()[, &quot;Package&quot;] for (pkg in packages) { if (!pkg %in% installed_pkgs) { install.packages(pkg) } } } install_all(packages) # load packages library(cobalt) library(data.table) library(dplyr) library(ggplot2) library(gridExtra) library(highr) library(highs) library(MatchIt) library(optmatch) library(optweight) library(quickmatch) library(readr) library(tidyr) library(tidyverse) library(WeightIt) 1.2 Wrapper functions Next, we outline the wrapper functions designed to address overlap and covariate balance as well as the estimation of the average treatment effect on the treated (ATT) and alternative estimands. To use the wrapper functions, the simplest method is to source the R script with the following code. source(&quot;tutorial/functions.R&quot;) If you are using RStudio, you should now see the functions in the environment section. 1.2.1 Overview Function Description Code inspect_data() Inspects datasets. More matchit_profile(),create_smr_weights(),create_overlap_weights(), truncate_weights_fixed(), truncate_weights_percentile(), check_weights(), truncate_weights_adaptive(), ps_trim(), common_range_trim(), crump_trim(), stuermer_trim (), walker_trim() and trim_attach_weights() | Implements methods to improve covariate balancing and overlap. | More | get_smd_stats(), compute_abs_smd_matchit(), compute_ess_matchit(), plot_matchit(), compute_abs_smd_weight(), compute_ess_weight(), plot_weighting_methods(), compute_abs_smd_trunc(), compute_ess_trunc(), plot_trunc_methods(), compute_abs_smd_trim(), compute_ess_trim(), plot_trim(), compute_smd_all_datasets (), compute_ess_all_datasets(), plot_comb_overlap(), plot_comb_love_plots(), combine_results(), save_csv(), assess_methods(), get_top_methods(), create_top5_datasets() and save_top5_individual_files() | Evaluates performance of previous methods through comparative metrics (absolute standardized mean differences (SMD) and effective sample size (ESS)) | More | | estimate_all(), plot_coef(), plot_att_panels(), save_att_panels(), create_matrix_results(), eval_att(), plot_catt_panels(), save_catt_panels(), eval_catt(), plot_qte_top(), save_qtet(), save_qte_top() | Computes and visualizes the Average Treatment Effect on the Treated (ATT) using a number of estimators. | More | | get_res_att(), derive_ow(), plot_ow(), eval_ow(), save_ow() | Analyses outcome weights of datasets. | More | | check_filter_datasets(), save_sensitivity_plots() | Validates and filters datasets. | More | 1.2.2 Inspection inspect_data() summarizes one or more datasets by returning a single data frame with each dataset’s name, number of observations, number of variables, and the concatenated names of its variables. inspect_data &lt;- function(data_list) { if (is.data.frame(data_list)) { data_list &lt;- list(dataset = data_list) } data.frame( dataset = names(data_list), num_obs = sapply(data_list, nrow), num_vars = sapply(data_list, ncol), name_vars = sapply(data_list, function(df) paste(names(df), collapse = &quot;, &quot;)), row.names = NULL ) } Argument data_list: The dataset or a list of datasets. 1.2.3 Improving covariate balance and overlap 1.2.3.1 Matching matchit_profile() performs nearest neighbor matching based on a combination of specified covariates and each observation’s distance from the overall covariate mean, returning a matchit object for balance diagnostics and matched sampling. matchit_profile &lt;- function(data, treat, covar) { overall_means &lt;- colMeans(data[, covar], na.rm = TRUE) cov_matrix &lt;- as.matrix(data[, covar]) dist_from_target &lt;- apply(cov_matrix, 1, function(x) sqrt(sum((x - overall_means)^2))) data$dist_from_target &lt;- dist_from_target formula &lt;- as.formula(paste(treat, &quot;~&quot;, paste(c(covar, &quot;dist_from_target&quot;), collapse = &quot;+&quot;))) match_out &lt;- matchit(formula, data = data, method = &quot;nearest&quot;, distance = &quot;glm&quot;) return(match_out) } Arguments data: The input dataset. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. 1.2.3.2 Weighting create_smr_weights() calculates standardized mortality ratio (SMR) weights based on estimated propensity scores, allowing for ATT or ATE weighting according to the estimand specified. create_smr_weights &lt;- function(data, formula, estimand = &quot;ATT&quot;) { ps_model &lt;- glm(formula, data = data, family = binomial()) ps &lt;- predict(ps_model, type = &quot;response&quot;) if (estimand == &quot;ATT&quot;) { weights &lt;- ifelse(data$treat == 1, 1/ps, 1/(1-ps)) } else if (estimand == &quot;ATE&quot;) { weights &lt;- ifelse(data$treat == 1, 1/ps, 1/(1-ps)) } return(weights) } Arguments data: The input dataset. formula: The model formula specifying the relation between the treatment variable and a set of covariates, defining the model structure. estimand = ATT: The target estimand (default is ATT). create_overlap_weights() computes overlap weights based on propensity scores. create_overlap_weights &lt;- function(data, formula) { ps_model &lt;- glm(formula, data = data, family = binomial()) ps &lt;- predict(ps_model, type = &quot;response&quot;) return(ifelse(data$treat == 1, 1 - ps, ps)) } Arguments data: The input dataset. formula: The model formula specifying the relation between the treatment variable and a set of covariates, defining the model structure. 1.2.3.3 Truncation truncate_weights_fixed() caps weights at a fixed maximum value to limit extreme weight influence. truncate_weights_fixed &lt;- function(data, weight_col, max_weight = 10) { data[[weight_col]] &lt;- pmin(data[[weight_col]], max_weight) return(data) } Arguments data: The input dataset. weight_col: The column name of the weights in the dataset. max_weight: The maximum allowed weight value. truncate_weights_percentile() caps weights at a specified percentile threshold to reduce outliers in weight distribution. truncate_weights_percentile &lt;- function(data, weight_col, percentile = 0.99) { weight_cutoff &lt;- quantile(data[[weight_col]], probs = percentile, na.rm = TRUE) data[[weight_col]] &lt;- pmin(data[[weight_col]], weight_cutoff) return(data) } Arguments data: The input dataset. weight_col: The column name of the weights in the dataset. percentile: The quantile cutoff value. check_weights() prints the variance of the specified weight vector to assess variability and relevance for adaptive truncation. check_weights &lt;- function(data, weight_col = &quot;weight&quot;) { w &lt;- data[[weight_col]] cat(&quot;Variance of&quot;, weight_col, &quot;:&quot;, var(w, na.rm = TRUE), &quot;\\n&quot;) } Arguments data: The input dataset. weight_col: The weight column to analyze. truncate_weights_adaptive() caps weights adaptively at a threshold defined by the mean plus a multiple of the standard deviation. truncate_weights_adaptive &lt;- function(data, weight_col, c = 3) { w &lt;- data[[weight_col]] # Only apply if variance is positive if (var(w, na.rm = TRUE) &gt; 0) { cutoff &lt;- mean(w, na.rm = TRUE) + c * sd(w, na.rm = TRUE) data[[weight_col]] &lt;- pmin(w, cutoff) } return(data) } Arguments data: The input dataset. weight_col: The column name of the weights in the dataset. c: The multiplier for the standard deviation added to the mean to define the cutoff value. 1.2.3.4 Trimming ps_trim() trims the dataset based on a specified propensity score threshold, removing observations whose propensity scores exceed the given cutoff value.*** ps_trim &lt;- function(data, ps = &quot;ps_assoverlap&quot;, threshold = 0.9) { sub &lt;- data[which(data[, ps] &lt; threshold), ] return(sub) } Arguments data: The input dataset. ps: The name of the propensity score variable. threshold: The upper limit for propensity score inclusion. common_range_trim() trims the dataset to observations with propensity scores within the common support range for treated and control groups. common_range_trim &lt;- function(data, ps_col = &quot;ps_assoverlap&quot;, treat_col = &quot;treat&quot;) { lower_cut &lt;- max( min(data[[ps_col]][data[[treat_col]] == 1], na.rm = TRUE), min(data[[ps_col]][data[[treat_col]] == 0], na.rm = TRUE) ) upper_cut &lt;- min( max(data[[ps_col]][data[[treat_col]] == 1], na.rm = TRUE), max(data[[ps_col]][data[[treat_col]] == 0], na.rm = TRUE) ) sub &lt;- data[data[[ps_col]] &gt;= lower_cut &amp; data[[ps_col]] &lt;= upper_cut, ] return(sub) } Arguments data: The input dataset. ps_col: The column name of propensity scores. treat_col: The column name of the (binary) treatment indicator. crump_trim() trims the dataset to observations with propensity scores outside specified lower and upper bounds (default 0.1 and 0.9). crump_trim &lt;- function(data, ps_col = &quot;ps_assoverlap&quot;, lower = 0.1, upper = 0.9) { sub &lt;- data[data[[ps_col]] &gt;= lower &amp; data[[ps_col]] &lt;= upper, ] return(sub) } Arguments data: The input dataset. ps_col: The column name of propensity scores. lower: The lower bound for inclusion. upper: The upper bound for inclusion. stuermer_trim () trims the dataset to observations based on propensity score quantiles separately for treated and control group. stuermer_trim &lt;- function(data, treat_col = &quot;treat&quot;, ps_col = &quot;ps_assoverlap&quot;, lower_percentile = 0.05, upper_percentile = 0.95) { treated_ps &lt;- data[[ps_col]][data[[treat_col]] == 1] untreated_ps &lt;- data[[ps_col]][data[[treat_col]] == 0] lower_cutoff &lt;- quantile(treated_ps, probs = lower_percentile, na.rm = TRUE) upper_cutoff &lt;- quantile(untreated_ps, probs = upper_percentile, na.rm = TRUE) sub &lt;- data[data[[ps_col]] &gt;= lower_cutoff &amp; data[[ps_col]] &lt;= upper_cutoff, ] return(sub) } Arguments data: The input dataset. treat_col: The column name of the (binary) treatment indicator. ps_col: The column name of propensity scores. walker_trim() trims the dataset to observations based on preference scores that adjust for treatment prevalence using logit transformations. walker_trim &lt;- function(data, treat_col = &quot;treat&quot;, ps_col = &quot;ps_assoverlap&quot;, lower_cutoff = 0.3, upper_cutoff = 0.7) { treat_prevalence &lt;- mean(data[[treat_col]], na.rm = TRUE) logit_ps &lt;- log(data[[ps_col]] / (1 - data[[ps_col]])) logit_prevalence &lt;- log(treat_prevalence / (1 - treat_prevalence)) preference_score &lt;- 1 / (1 + exp(-(logit_ps - logit_prevalence))) sub &lt;- data[preference_score &gt;= lower_cutoff &amp; preference_score &lt;= upper_cutoff, ] return(sub) } Arguments data: The input dataset. treat_col: The column name of the (binary) treatment indicator. ps_col: The column name of propensity scores. 1.2.3.5 Combined methods trim_attach_weights() merges trimmed data with original weights, preserving corresponding weight values after trimming. trim_attach_weights &lt;- function(trimmed_data, original_data, weight_col){ trimmed_data$orig_row &lt;- as.integer(rownames(trimmed_data)) original_data$orig_row &lt;- as.integer(rownames(original_data)) trimmed_data &lt;- merge( trimmed_data, original_data[, c(&quot;orig_row&quot;, weight_col)], by = &quot;orig_row&quot;, all.x = TRUE ) colnames(trimmed_data)[colnames(trimmed_data) == weight_col] &lt;- &quot;weight&quot; trimmed_data$orig_row &lt;- NULL original_data$orig_row &lt;- NULL return(trimmed_data) } Arguments trimmed_data: The dataset resulting from trimming procedures, typically a subset of the original dataset. original_data: The full dataset before trimming. weight_col: The name of the column in original_data that holds the weights to be transferred. 1.2.4 Reassessing methods 1.2.4.1 Matching get_smd_stats() extracts mean and maximum absolute standardized mean differences (SMDs) from a balance object to assess covariate balance. get_smd_stats &lt;- function(match_object) { bal &lt;- bal.tab(match_object, stats = &quot;mean.diffs&quot;, un = TRUE, s.d.denom = &quot;treated&quot;) smds &lt;- bal$Balance$Diff.Adj smds &lt;- smds[!(rownames(bal$Balance) %in% c(&quot;distance&quot;))] mean_smd &lt;- mean(abs(smds), na.rm = TRUE) max_smd &lt;- max(abs(smds), na.rm = TRUE) return(c(Mean_Abs_SMD = mean_smd, Max_Abs_SMD = max_smd)) } Argument match_object: The balance / matching object, containing information on covariate balance. compute_abs_smd_matchit() computes absolute SMDs for a list of MatchIt objects and returns summary statistics. compute_abs_smd_matchit &lt;- function(match_list) { smd_list &lt;- lapply(match_list, get_smd_stats) smd_mat &lt;- do.call(rbind, smd_list) smd_df &lt;- data.frame( Method = names(match_list), Mean_Abs_SMD = smd_mat[, &quot;Mean_Abs_SMD&quot;], Max_Abs_SMD = smd_mat[, &quot;Max_Abs_SMD&quot;], row.names = NULL ) return(smd_df) } Argument match_list: A named list of MatchIt objects. compute_ess_matchit() extracts and formats effective sample size (ESS) information from the balance summary of a MatchIt object. compute_ess_matchit &lt;- function(bal_tab_object) { samples &lt;- bal_tab_object$Observations df &lt;- as.data.frame(samples) df$Method &lt;- rownames(samples) df &lt;- df[, c(&quot;Method&quot;,&quot;Control&quot;, &quot;Treated&quot;)] rownames(df) &lt;- NULL df } Argument bal_tab_object: A balance table object that contains ESS information from matched data. plot_matchit() generates balance plots for a list of MatchIt objects, visualizing covariate balance before and after matching. plot_matchit &lt;- function(match_list, dataset_name) { for (method_name in names(match_list)) { match_obj &lt;- match_list[[method_name]] if (method_name == &quot;subcl&quot;) { plot(summary(match_obj, subclass = TRUE), main = paste0(dataset_name, &quot; - &quot;, method_name,&quot; matching&quot;)) } else { plot(summary(match_obj), main = paste0(dataset_name, &quot; - &quot;, method_name,&quot; matching&quot;)) } } } Arguments match_list: A named list of MatchIt objects. dataset_name: A string naming the dataset for use in plot titles. 1.2.4.2 Weighting compute_abs_smd_weight() calculates absolute SMDs for datasets weighted by specified weight vectors. compute_abs_smd_weight &lt;- function(data, treat, covar, weight_cols) { smd_list &lt;- lapply(weight_cols, function(wcol) { bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = data, weights = data[[wcol]], un = TRUE, s.d.denom = &quot;treated&quot; ) smd_df &lt;- as.data.frame(bal_obj$Balance) smd_vals &lt;- abs(smd_df$Diff.Adj) mean_smd &lt;- mean(smd_vals, na.rm = TRUE) max_smd &lt;- max(smd_vals, na.rm = TRUE) return(data.frame( Method = wcol, Mean_Abs_SMD = mean_smd, Max_Abs_SMD = max_smd )) }) do.call(rbind, smd_list) } Arguments data: The input dataset. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. weight_cols:The name of the columns holding the weights. compute_ess_weight() extracts ESS associated with different weighting schemes. compute_ess_weight &lt;- function(data, treat, covar, weight_cols) { ess_list &lt;- lapply(weight_cols, function(wcol) { bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = data, weights = data[[wcol]], un = FALSE ) samples &lt;- bal_obj$Observations if (&quot;Adjusted&quot; %in% rownames(samples)) { df &lt;- samples[&quot;Adjusted&quot;, c(&quot;Control&quot;, &quot;Treated&quot;), drop = FALSE] } else { df &lt;- samples[1, c(&quot;Control&quot;, &quot;Treated&quot;), drop = FALSE] } df &lt;- cbind(Method = wcol, df) rownames(df) &lt;- NULL return(df) }) do.call(rbind, ess_list) } Arguments data: The input dataset. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. weight_cols: The name of the columns holding the weights. plot_weighting_methods() generates love plots, visualizing covariate balance under different weighting methods. plot_weighting_methods &lt;- function(data, treat, covar, weight_list, dataset_name = NULL) { for (wcol in names(weight_list)) { bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = data, weights = weight_list[[wcol]], un = TRUE, s.d.denom = &quot;treated&quot; ) title_text &lt;- if (!is.null(dataset_name)) paste(dataset_name, &quot;-&quot;, wcol, &quot;weighting&quot;) else wcol lp &lt;- cobalt::love.plot( bal_obj, stats = &quot;mean.diffs&quot;, abs = TRUE, var.order = &quot;unadjusted&quot;, thresholds = c(m = 0.1), title = title_text ) print(lp) } } Arguments data: The input dataset. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. weight_list: A named list of weight vectors. dataset_name: The dataset name for plot titles. 1.2.4.3 Truncation compute_abs_smd_trunc() calculates absolute SMDs for truncated weight datasets across different truncation methods and weighting variables. compute_abs_smd_trunc &lt;- function(trunc_list, treat, covar, weight_cols) { all_smd &lt;- list() for(trunc_name in names(trunc_list)) { dataset &lt;- trunc_list[[trunc_name]] smd_list &lt;- lapply(weight_cols, function(wcol) { if (wcol %in% names(dataset)) { bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = dataset, weights = dataset[[wcol]], un = TRUE, s.d.denom = &quot;treated&quot; ) smd_df &lt;- as.data.frame(bal_obj$Balance) smd_vals &lt;- abs(smd_df$Diff.Adj) data.frame( Method = paste(trunc_name, wcol, sep = &quot;_&quot;), Mean_Abs_SMD = mean(smd_vals, na.rm = TRUE), Max_Abs_SMD = max(smd_vals, na.rm = TRUE) ) } else { NULL } }) all_smd[[trunc_name]] &lt;- do.call(rbind, smd_list) } smd_summary &lt;- do.call(rbind, all_smd) rownames(smd_summary) &lt;- NULL return(smd_summary) } Arguments trunc_list: A list of datasets with truncated weights. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. weight_cols: The name of the columns holding the weights. compute_ess_trunc() computes ESS values for truncated weight datasets considering different methods and weights. compute_ess_trunc &lt;- function(trunc_list, treat, covar, weight_cols) { all_ess &lt;- list() for(trunc_name in names(trunc_list)) { dataset &lt;- trunc_list[[trunc_name]] ess_list &lt;- lapply(weight_cols, function(wcol) { if (wcol %in% names(dataset)) { bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = dataset, weights = dataset[[wcol]], un = FALSE, s.d.denom = &quot;treated&quot; ) samples &lt;- bal_obj$Observations if (&quot;Adjusted&quot; %in% rownames(samples)) { df &lt;- data.frame( Method = paste(trunc_name, wcol, sep = &quot;_&quot;), Control = samples[&quot;Adjusted&quot;, &quot;Control&quot;], Treated = samples[&quot;Adjusted&quot;, &quot;Treated&quot;] ) } else { df &lt;- data.frame( Method = paste(trunc_name, wcol, sep = &quot;_&quot;), Control = samples[1, &quot;Control&quot;], Treated = samples[1, &quot;Treated&quot;] ) } df } else { NULL } }) all_ess[[trunc_name]] &lt;- do.call(rbind, ess_list) } ess_summary &lt;- do.call(rbind, all_ess) rownames(ess_summary) &lt;- NULL return(ess_summary) } Arguments trunc_list: A list of truncated weight datasets. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. weight_cols: The name of the columns holding the weights. plot_trunc_methods() produces love plots for various truncation methods and weight types, visualizing covariate balance before and after truncation. plot_trunc_methods &lt;- function(trunc_list, treat, covar, weight_cols, dataset_name = NULL) { for(trunc_name in names(trunc_list)) { dataset &lt;- trunc_list[[trunc_name]] for(wcol in weight_cols) { if(wcol %in% names(dataset)) { bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = dataset, weights = dataset[[wcol]], un = TRUE, s.d.denom = &quot;treated&quot; ) title_text &lt;- if (!is.null(dataset_name)) { paste(dataset_name, &quot;-&quot;, trunc_name, &quot;-&quot;, wcol, &quot;truncation&quot;) } else { paste(trunc_name, wcol, &quot;truncation&quot;) } lp &lt;- cobalt::love.plot( bal_obj, stats = &quot;mean.diffs&quot;, abs = TRUE, var.order = &quot;unadjusted&quot;, thresholds = c(m = 0.1), stars = &quot;none&quot;, title = title_text ) print(lp) } } } } Arguments trunc_list: A list of truncated weight datasets. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. weight_cols: The name of the columns holding the weights. dataset_name: The dataset name for plot titles. 1.2.4.4 Trimming compute_abs_smd_trim() computes absolute SMDs for multiple trimmed datasets. compute_abs_smd_trim &lt;- function(trimming_list, treat, covar) { smd_list &lt;- lapply(names(trimming_list), function(name) { data &lt;- trimming_list[[name]] bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = data, un = TRUE, s.d.denom = &quot;treated&quot; ) smd_df &lt;- as.data.frame(bal_obj$Balance) smd_vals &lt;- abs(smd_df$Diff.Un) mean_smd &lt;- mean(smd_vals, na.rm = TRUE) max_smd &lt;- max(smd_vals, na.rm = TRUE) return(data.frame( Method = name, Mean_Abs_SMD = mean_smd, Max_Abs_SMD = max_smd )) }) smd_summary &lt;- do.call(rbind, smd_list) rownames(smd_summary) &lt;- NULL return(smd_summary) } Arguments trimming_list: A named list of trimmed datasets. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. compute_ess_trim() extracts ESS information for multiple trimmed datasets. compute_ess_trim &lt;- function(trimming_list, treat, covar) { ess_list &lt;- lapply(trimming_list, function(data) { bal_obj &lt;- cobalt::bal.tab(as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = data, un = TRUE) samples &lt;- bal_obj$Observations df &lt;- as.data.frame(samples)[c(&quot;Control&quot;, &quot;Treated&quot;)] return(df) }) ess_df &lt;- do.call(rbind, ess_list) ess_df$Method &lt;- names(trimming_list) rownames(ess_df) &lt;- NULL ess_df &lt;- ess_df[, c(&quot;Method&quot;, &quot;Control&quot;, &quot;Treated&quot;)] return(ess_df) } Arguments trimming_list: A named list of trimmed datasets. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. plot_trim() plots overlap diagnostics for a set of trimmed datasets, visualizing covariate balance before and after matching. plot_trim &lt;- function(data_list, treat, covar) { par(mfrow = c(2,3)) for (data_obj in data_list) { assess_overlap(data_obj, treat = treat, cov = covar) } } Arguments data_list: A list of datasets to plot. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. 1.2.4.5 Combined methods compute_smd_all_datasets () aggregates absolute SMD statistics across multiple datasets, weighting methods, and trimming techniques. compute_smd_all_datasets &lt;- function(combined_list, treat, covar) { smd_list &lt;- lapply(names(combined_list), function(weight_method) { method_list &lt;- combined_list[[weight_method]] res &lt;- lapply(names(method_list), function(trim_method) { data &lt;- method_list[[trim_method]] # Use equal weights if weight column missing if (!&quot;weight&quot; %in% colnames(data)) { data$weight &lt;- rep(1, nrow(data)) } bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = data, weights = data$weight, un = TRUE, s.d.denom = &quot;treated&quot; ) smd_df &lt;- as.data.frame(bal_obj$Balance) smd_vals &lt;- abs(smd_df$Diff.Adj) method_name &lt;- paste(weight_method, trim_method, sep = &quot;_&quot;) data.frame( Method = method_name, Mean_Abs_SMD = mean(smd_vals, na.rm = TRUE), Max_Abs_SMD = max(smd_vals, na.rm = TRUE) ) }) do.call(rbind, res) }) smd_summary &lt;- do.call(rbind, smd_list) rownames(smd_summary) &lt;- NULL return(smd_summary) } Arguments combined_list: A nested list of datasets grouped by weighting and trimming methods. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. compute_ess_all_datasets() aggregates ESS information across various datasets, weighting methods, and trimming techniques. compute_ess_all_datasets &lt;- function(combined_list, treat, covar) { ess_list &lt;- lapply(names(combined_list), function(weight_method) { method_list &lt;- combined_list[[weight_method]] res &lt;- lapply(names(method_list), function(trim_method) { data &lt;- method_list[[trim_method]] # Use equal weights if weight column missing if (!&quot;weight&quot; %in% colnames(data)) { data$weight &lt;- rep(1, nrow(data)) } bal_obj &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = data, weights = data$weight, un = TRUE, s.d.denom = &quot;treated&quot; ) samples &lt;- bal_obj$Observations if (&quot;Adjusted&quot; %in% rownames(samples)) { control_ess &lt;- samples[&quot;Adjusted&quot;, &quot;Control&quot;] treated_ess &lt;- samples[&quot;Adjusted&quot;, &quot;Treated&quot;] } else { control_ess &lt;- samples[1, &quot;Control&quot;] treated_ess &lt;- samples[1, &quot;Treated&quot;] } method_name &lt;- paste(weight_method, trim_method, sep = &quot;_&quot;) data.frame( Method = method_name, Control = control_ess, Treated = treated_ess ) }) do.call(rbind, res) }) ess_summary &lt;- do.call(rbind, ess_list) rownames(ess_summary) &lt;- NULL return(ess_summary) } Arguments combined_list: A nested list of datasets grouped by weighting and trimming methods. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. plot_comb_overlap() creates plots assessing overlap for combinations of weighting and trimming methods across datasets. plot_comb_overlap &lt;- function(comb_meth_cps = NULL, comb_meth_psid = NULL, treat, covar, prefix_cps = &quot;LDW-CPS1&quot;, prefix_psid = &quot;LDW-PSID1&quot;) { all_combined_list &lt;- list() if (!is.null(comb_meth_cps)) all_combined_list$CPS &lt;- comb_meth_cps if (!is.null(comb_meth_psid)) all_combined_list$PSID &lt;- comb_meth_psid for (ds_name in names(all_combined_list)) { combined_list &lt;- all_combined_list[[ds_name]] plot_list &lt;- list() method_names &lt;- character() for (weight_method in names(combined_list)) { for (trim_method in names(combined_list[[weight_method]])) { full_method_name &lt;- paste(weight_method, trim_method, sep = &quot;_&quot;) plot_list[[full_method_name]] &lt;- list( data = combined_list[[weight_method]][[trim_method]], method_name = full_method_name ) method_names &lt;- c(method_names, full_method_name) } } total_plots &lt;- length(plot_list) plots_per_page &lt;- 4 # 2x2 layout for (i in seq(1, total_plots, by = plots_per_page)) { page_plots &lt;- plot_list[i:min(i + plots_per_page - 1, total_plots)] par(mfrow = c(2, 2), mar = c(4, 4, 2, 1)) for (p in page_plots) { assess_overlap(p$data, treat = treat, cov = covar) idx &lt;- match(p$method_name, method_names) prefix &lt;- if (ds_name == &quot;CPS&quot; &amp;&amp; idx &lt;= 25) prefix_cps else if (ds_name == &quot;PSID&quot; &amp;&amp; idx &lt;= 25) prefix_psid else &quot;&quot; title(main = paste0(prefix, &quot; - &quot;, p$method_name)) } } } } Arguments comb_meth_cps: A named list combining weighting and trimming results for CPS dataset. comb_meth_psid: A named list combining weighting and trimming results for PSID dataset. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. prefix_cps: Optional string prefix for plot titles related to CPS results. prefix_psid: Optional string prefix for plot titles related to PSID results. plot_comb_love_plots() visualizes SMDs via love plots for combined weighting and trimming approaches. plot_comb_love_plots &lt;- function(comb_meth_cps = NULL, comb_meth_psid = NULL, treat, covar, prefix_cps = &quot;LDW-CPS1&quot;, prefix_psid = &quot;LDW-PSID1&quot;) { all_datasets &lt;- list() if (!is.null(comb_meth_cps)) all_datasets$CPS &lt;- comb_meth_cps if (!is.null(comb_meth_psid)) all_datasets$PSID &lt;- comb_meth_psid for (ds_name in names(all_datasets)) { method_list &lt;- all_datasets[[ds_name]] method_names &lt;- unlist(lapply(names(method_list), function(weighting) { trimmed_list &lt;- method_list[[weighting]] sapply(names(trimmed_list), function(trim) paste(weighting, trim, sep = &quot;_&quot;)) })) plot_counter &lt;- 0 for (weighting in names(method_list)) { trimmed_list &lt;- method_list[[weighting]] for (trim in names(trimmed_list)) { df &lt;- trimmed_list[[trim]] plot_counter &lt;- plot_counter + 1 if (!&quot;weight&quot; %in% names(df)) df$weight &lt;- 1 bal &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = df, weights = df$weight, un = TRUE, s.d.denom = &quot;treated&quot; ) method_name &lt;- paste(weighting, trim, sep = &quot;_&quot;) prefix &lt;- &quot;&quot; if (ds_name == &quot;CPS&quot; &amp;&amp; plot_counter &lt;= 25) prefix &lt;- prefix_cps if (ds_name == &quot;PSID&quot; &amp;&amp; plot_counter &lt;= 25) prefix &lt;- prefix_psid title_text &lt;- paste(prefix, method_name, sep = &quot; - &quot;) lp &lt;- cobalt::love.plot( bal, stats = &quot;mean.diffs&quot;, absolute = TRUE, var.order = &quot;unadjusted&quot;, thresholds = c(m = .1), title = title_text ) print(lp) } } } } Arguments comb_meth_cps: A named list combining weighting and trimming results for CPS dataset. comb_meth_psid: A named list combining weighting and trimming results for PSID dataset. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. prefix_cps: Optional string prefix for plot titles related to CPS results. prefix_psid: Optional string prefix for plot titles related to PSID results. save_comb_hist() saves the histograms into a pdf file. save_comb_hist &lt;- function(comb_meth_cps = NULL, comb_meth_psid = NULL, treat, covar, prefix = &quot;model_a&quot;, prefix_cps = &quot;LDW-CPS1&quot;, prefix_psid = &quot;LDW-PSID1&quot;, path = &quot;graphs/lalonde&quot;) { dir.create(path, showWarnings = FALSE, recursive = TRUE) all_combined_list &lt;- list() if (!is.null(comb_meth_cps)) all_combined_list$CPS &lt;- comb_meth_cps if (!is.null(comb_meth_psid)) all_combined_list$PSID &lt;- comb_meth_psid file_index &lt;- 1 for (ds_name in names(all_combined_list)) { combined_list &lt;- all_combined_list[[ds_name]] method_names &lt;- character() plot_list &lt;- list() for (weight_method in names(combined_list)) { for (trim_method in names(combined_list[[weight_method]])) { full_method_name &lt;- paste(weight_method, trim_method, sep = &quot;_&quot;) plot_list[[full_method_name]] &lt;- combined_list[[weight_method]][[trim_method]] method_names &lt;- c(method_names, full_method_name) } } total_plots &lt;- length(plot_list) for (i in seq_len(total_plots)) { data &lt;- plot_list[[i]] pdf_file &lt;- file.path(path, sprintf(&quot;%s_overlap_%d.pdf&quot;, prefix, file_index)) pdf(pdf_file, width = 8, height = 6) assess_overlap(data, treat = treat, cov = covar) prefix_str &lt;- if (ds_name == &quot;CPS&quot;) prefix_cps else prefix_psid title(main = paste0(prefix_str, &quot; - &quot;, method_names[i])) dev.off() file_index &lt;- file_index + 1 } } } Arguments comb_meth_cps: A named list combining weighting and trimming results for CPS dataset. comb_meth_psid: A named list combining weighting and trimming results for PSID dataset. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. prefix: Optional string prefix for plot titles related to the respective dataset or model. prefix_cps: Optional string prefix for plot titles related to CPS results. prefix_psid: Optional string prefix for plot titles related to PSID results. save_comb_loveplots() saves the loveplots into a pdf file. save_comb_loveplots &lt;- function(comb_meth_cps = NULL, comb_meth_psid = NULL, treat, covar, prefix = &quot;model_a&quot;, prefix_cps = &quot;LDW-CPS1&quot;, prefix_psid = &quot;LDW-PSID1&quot;, path = &quot;graphs/lalonde&quot;) { dir.create(path, showWarnings = FALSE, recursive = TRUE) all_datasets &lt;- list() if (!is.null(comb_meth_cps)) all_datasets$CPS &lt;- comb_meth_cps if (!is.null(comb_meth_psid)) all_datasets$PSID &lt;- comb_meth_psid file_index &lt;- 1 for (ds_name in names(all_datasets)) { method_list &lt;- all_datasets[[ds_name]] method_names &lt;- unlist(lapply(names(method_list), function(weighting) { trimmed_list &lt;- method_list[[weighting]] sapply(names(trimmed_list), function(trim) paste(weighting, trim, sep = &quot;_&quot;)) })) for (weighting in names(method_list)) { trimmed_list &lt;- method_list[[weighting]] for (trim in names(trimmed_list)) { df &lt;- trimmed_list[[trim]] if (!&quot;weight&quot; %in% names(df)) df$weight &lt;- 1 bal &lt;- cobalt::bal.tab( as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))), data = df, weights = df$weight, un = TRUE, s.d.denom = &quot;treated&quot; ) method_name &lt;- paste(weighting, trim, sep = &quot;_&quot;) prefix_str &lt;- ifelse(ds_name == &quot;CPS&quot;, prefix_cps, prefix_psid) pdf_file &lt;- file.path(path, sprintf(&quot;%s_balance_%d.pdf&quot;, prefix, file_index)) pdf(pdf_file, width = 8, height = 6) lp &lt;- cobalt::love.plot( bal, stats = &quot;mean.diffs&quot;, absolute = TRUE, var.order = &quot;unadjusted&quot;, thresholds = c(m = .1), title = paste(prefix_str, method_name, sep = &quot; - &quot;) ) print(lp) dev.off() file_index &lt;- file_index + 1 } } } } Arguments comb_meth_cps: A named list combining weighting and trimming results for CPS dataset. comb_meth_psid: A named list combining weighting and trimming results for PSID dataset. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. prefix: String prefix used for saved file names to identify dataset. prefix_cps: Optional string prefix for plot titles related to CPS results. prefix_psid: Optional string prefix for plot titles related to PSID results. 1.2.4.6 Getting top methods and datasets combine_results() merges and summarizes SMD and ESS statistics from various analysis stages into a final comparison table. combine_results &lt;- function(dataset_name) { dataset_lower &lt;- tolower(dataset_name) # Retrieve matching results smd_matching &lt;- get(paste0(&quot;smd_matchit.&quot;, dataset_lower)) ess_matching &lt;- get(paste0(&quot;ess_matchit.&quot;, dataset_lower)) # Retrieve trimming results smd_trimming &lt;- get(paste0(&quot;smd_trim.&quot;, dataset_lower)) ess_trimming &lt;- get(paste0(&quot;ess_trim.&quot;, dataset_lower)) # Retrieve truncation results smd_trunc &lt;- get(paste0(&quot;smd_trunc.&quot;, dataset_lower)) ess_trunc &lt;- get(paste0(&quot;ess_trunc.&quot;, dataset_lower)) # Retrieve weighting results smd_weighting &lt;- get(paste0(&quot;smd_weight.&quot;, dataset_lower)) ess_weighting &lt;- get(paste0(&quot;ess_weight.&quot;, dataset_lower)) # Retrieve combined SMD and ESS # Make sure you have these variables in your environment per dataset smd_combined_var &lt;- paste0(&quot;smd_all_comb_meth.&quot;, dataset_lower) ess_combined_var &lt;- paste0(&quot;ess_all_comb_meth.&quot;, dataset_lower) smd_combined &lt;- get(smd_combined_var) ess_combined &lt;- get(ess_combined_var) # Combine all SMD results smd_all &lt;- do.call(rbind, list( smd_matching, smd_trimming, smd_trunc, smd_weighting, smd_combined[, c(&quot;Method&quot;, &quot;Mean_Abs_SMD&quot;, &quot;Max_Abs_SMD&quot;)] )) # Combine all ESS results ess_all &lt;- do.call(rbind, list( ess_matching, ess_trimming, ess_trunc, ess_weighting, ess_combined[, c(&quot;Method&quot;, &quot;Control&quot;, &quot;Treated&quot;)] )) # Merge SMD and ESS results by Method final_df &lt;- merge(smd_all, ess_all, by = &quot;Method&quot;, all = TRUE) # Remove dataset suffixes like &quot;.cps_plus&quot; or &quot;.psid_plus&quot; from Method names for cleaner labels final_df$Method &lt;- gsub(&quot;\\\\.psid_plus&quot;, &quot;&quot;, final_df$Method, ignore.case = TRUE) final_df$Method &lt;- gsub(&quot;\\\\.cps_plus&quot;, &quot;&quot;, final_df$Method, ignore.case = TRUE) # Reset row names rownames(final_df) &lt;- NULL return(final_df) } Arguments dataset_name: The dataset name for which to combine SMD and ESS summaries. save_csv() saves the tables into a CSV file. save_csv &lt;- function(data, filename) { folder &lt;- &quot;../tables&quot; if (!dir.exists(folder)) dir.create(folder, recursive = TRUE) file_csv &lt;- file.path(folder, paste0(filename, &quot;.csv&quot;)) write.csv(data, file = file_csv, row.names = FALSE) } Arguments data: The input dataset. filename: The name of the file that is to be saved as CSV. assess_methods() scores and ranks methods based on a weighted combination of balance and sample size metrics. assess_methods &lt;- function(df) { df %&gt;% mutate( # ess score ess_balance_ratio = pmin(Control, Treated) / pmax(Control, Treated), ess_total = Control + Treated, # Normalize balance_ratio and ess_total ess_balance_score = (ess_balance_ratio - min(ess_balance_ratio, na.rm = TRUE)) / (max(ess_balance_ratio, na.rm = TRUE) - min(ess_balance_ratio, na.rm = TRUE)), ess_size_score = (ess_total - min(ess_total, na.rm = TRUE)) / (max(ess_total, na.rm = TRUE) - min(ess_total, na.rm = TRUE)), # Combine size and balance equally ess_score = 0.5 * ess_balance_score + 0.5 * ess_size_score, # smd score smd_score = 1 - (Mean_Abs_SMD - min(Mean_Abs_SMD, na.rm = TRUE)) / (max(Mean_Abs_SMD, na.rm = TRUE) - min(Mean_Abs_SMD, na.rm = TRUE)), # final score Score = 0.5 * smd_score + 0.5 * ess_score ) %&gt;% dplyr::select(Method, Score) %&gt;% arrange(desc(Score)) } Argument df: A data frame containing method performance metrics. get_top_methods() retrieves the top-performing methods based on combined scoring criteria. get_top_methods &lt;- function(summary_df, top_n = 5) { if (!all(c(&quot;Method&quot;, &quot;Score&quot;) %in% names(summary_df))) { stop(&quot;Data frame must contain columns &#39;Method&#39; and &#39;Score&#39;&quot;) } top_methods_df &lt;- summary_df %&gt;% arrange(desc(Score)) %&gt;% head(top_n) print(top_methods_df) return(top_methods_df$Method) } Arguments summary_df: A data frame including columns used for ranking. top_n: Integer specifying the number of top-performing methods to return (default 5). create_top5_datasets() extracts datasets corresponding to the top five ranked methods for further analysis or presentation. create_top5_datasets &lt;- function(dataset_list, top5_method_names) { lapply(top5_method_names, function(method_name) { if (!method_name %in% names(dataset_list)) { stop(paste0(&quot;Method &#39;&quot;, method_name, &quot;&#39; not found in the dataset lookup list&quot;)) } ds &lt;- dataset_list[[method_name]] if (inherits(ds, &quot;matchit&quot;)) { # for matchit class, extract matched data return(as.data.frame(match.data(ds))) } else if (is.data.frame(ds)) { # if already dataframe, return as is return(ds) } else if (is.vector(ds)) { # merge with original data if (!&quot;original&quot; %in% names(dataset_list)) { stop(&quot;Original dataset not found in dataset_list to merge weights&quot;) } original_df &lt;- dataset_list[[&quot;original&quot;]] if (length(ds) != nrow(original_df)) { stop(paste0(&quot;Length of weight vector for method &#39;&quot;, method_name, &quot;&#39; does not match number of rows in original dataset&quot;)) } # create new data frame with weights appended weighted_df &lt;- original_df weighted_df[[&quot;weights&quot;]] &lt;- ds return(weighted_df) } else { stop(paste(&quot;Unsupported data type for method&quot;, method_name)) } }) } Arguments dataset_list: A named list of datasets or MatchIt objects containing processed results for different methods. top5_method_names: A character vector of method names corresponding to the top-ranked datasets. save_top5_individual_files() saves datasets of the top five methods as individual files for reproducibility or sharing. save_top5_individual_files &lt;- function(combined_methods_list, top5_method_names, prefix) { for (i in seq_along(top5_method_names)) { method_name &lt;- top5_method_names[i] if (!method_name %in% names(combined_methods_list)) { warning(paste0(&quot;Method &#39;&quot;, method_name, &quot;&#39; not found in combined methods list&quot;)) next } dataset_to_save &lt;- combined_methods_list[[method_name]] file_name &lt;- sprintf(&quot;data/top%d_%s_method_%s.RData&quot;, i, prefix, method_name) save(dataset_to_save, file = file_name) cat(&quot;Saved:&quot;, file_name, &quot;\\n&quot;) } } Arguments combined_methods_list: A named list of all combined method results. top5_method_names: A character vector of method names corresponding to the top-ranked datasets. prefix: String prefix used for saved file names to identify dataset. 1.2.5 Estimating 1.2.5.1 Average treatment effect of the treated (ATT) estimate_all() runs multiple treatment effect estimators on a dataset and returns point estimates, standard errors and confidence intervals. # diff diff &lt;- function(data, Y, treat) { fml &lt;- as.formula(paste(Y, &quot;~&quot;, treat)) out &lt;- summary(lm_robust(fml, data = data, se_type = &quot;stata&quot;))$coefficients[treat, c(1, 2, 5, 6)] return(out) # extract coef, se, ci.lower, ci.upper } # reg reg &lt;- function(data, Y, treat, covar) { fml &lt;- as.formula(paste(Y, &quot;~&quot;, treat, &quot;+&quot;, paste(covar, collapse = &quot; + &quot;))) out &lt;- summary(lm_robust(fml, data = data, se_type = &quot;stata&quot;))$coefficients[treat, c(1, 2, 5, 6)] # extract coef, se, ci.lower, ci.upper return(out) } # matching matching &lt;- function(data, Y, treat, covar) { m.out &lt;- Match(Y = data[, Y], Tr = data[, treat], X = data[, covar], Z = data[, covar], estimand = &quot;ATT&quot;, M = 5, replace = TRUE, ties = TRUE, BiasAdjust = TRUE) out &lt;- c(m.out$est[1], m.out$se[1], m.out$est[1] - 1.96 * m.out$se[1], m.out$est[1] + 1.96 * m.out$se[1]) return(out) } # psm psm &lt;- function(data, Y, treat, covar) { ps &lt;- probability_forest(X = data[, covar], Y = as.factor(data[,treat]), seed = 1234, num.trees = 4000)$predictions[,2] m.out &lt;- Match(Y = data[, Y], Tr = data[, treat], X = matrix(ps, nrow(data), 1), estimand = &quot;ATT&quot;, M = 5, replace = TRUE, ties = FALSE, BiasAdjust = FALSE) if (is.null(m.out$se)==FALSE) { se &lt;- m.out$se[1] } else { se &lt;- m.out$se.standard[1] } out &lt;- c(m.out$est[1], se, m.out$est[1] - 1.96 * se, m.out$est[1] + 1.96 * se) return(out) } # OM (reg) om.reg &lt;- function(data, Y, treat, covar) { tr &lt;- which(data[, treat] == 1) co &lt;- which(data[, treat] == 0) fml &lt;- as.formula(paste(Y, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))) out.co &lt;- lm(fml, data = data[co, ]) Y.tr.hat &lt;- predict(out.co, newdata = data[tr, covar, drop = FALSE]) newdata &lt;- cbind.data.frame(Y = c(data[tr, Y], Y.tr.hat), treat = rep(c(1, 0), each = length(tr))) out &lt;- summary(lm_robust(Y ~ treat, data = newdata, se_type = &quot;stata&quot;))$coefficients[&quot;treat&quot;, c(1, 2, 5, 6)] return(out) } # OM (grf) om.grf &lt;- function(data, Y, treat, covar) { tr &lt;- which(data[, treat] == 1) co &lt;- which(data[, treat] == 0) out.co &lt;- regression_forest(X = data[co, covar, drop = FALSE], Y = as.vector(data[co, Y]) ) Y.tr.hat &lt;- as.vector(unlist(predict(out.co, newdata = data[tr, covar, drop = FALSE]))) newdata &lt;- cbind.data.frame(Y = c(data[tr, Y], Y.tr.hat), treat = rep(c(1, 0), each = length(tr))) out &lt;- summary(lm_robust(Y ~ treat, data = newdata, se_type = &quot;stata&quot;))$coefficients[&quot;treat&quot;, c(1, 2, 5, 6)] return(out) } # IPW ipw &lt;- function(data, Y, treat, covar) { ps &lt;- probability_forest(X = data[, covar, drop = FALSE], Y = as.factor(data[, treat]), seed = 1234)$predictions[,2] fml &lt;- as.formula(paste(Y, &quot;~&quot;, treat)) weights &lt;- rep(1, nrow(data)) co &lt;- which(data[, treat] == 0) weights[co] &lt;- ps[co]/(1-ps[co]) out &lt;- summary(lm_robust(fml, data = data, weights = weights, se_type = &quot;stata&quot;))$coefficients[treat, c(1, 2, 5, 6)] # extract coef, se, ci.lower, ci.upper return(out) } # CBPS #library(&quot;CBPS&quot;) cbps &lt;- function(data, Y, treat, covar) { fml &lt;- as.formula(paste(treat, &quot;~&quot;, paste(covar, collapse = &quot; + &quot;))) ps &lt;- quiet(CBPS(fml, data = data, standardize = TRUE)$fitted.values) fml &lt;- as.formula(paste(Y, &quot;~&quot;, treat)) weights &lt;- rep(1, nrow(data)) co &lt;- which(data[, treat] == 0) weights[co] &lt;- ps[co]/(1-ps[co]) out &lt;- summary(lm_robust(fml, data = data, weights = weights, se_type = &quot;stata&quot;))$coefficients[treat, c(1, 2, 5, 6)] return(out) } # ebal #library(hbal) ebal &lt;- function(data, Y, treat, covar) { ebal.out &lt;- hbal::hbal(Y = Y, Treat = treat, X = covar, data = data, expand.degree = 1) out &lt;- hbal::att(ebal.out, dr = FALSE)[1, c(1, 2, 5, 6)] return(out) } # hbal # hbal &lt;- function(data, Y, treat, covar) { # hbal.out &lt;- hbal::hbal(Y = Y, Treat = treat, X = covar, data = data, expand.degree = 2, # cv = TRUE) # out &lt;- hbal::att(hbal.out, dr = FALSE)[1, c(1, 2, 5, 6)] # return(out) # } # aipw_grf aipw &lt;- function(data, Y, treat, covar) { #library(&quot;grf&quot;) for (var in c(Y, treat, covar)) { data[, var] &lt;- as.vector(data[, var]) } c.forest &lt;- causal_forest(X = data[, covar, drop = FALSE], Y = data[, Y], W = data[, treat], seed = 1234) att &lt;- average_treatment_effect(c.forest, target.sample = &quot;treated&quot;, method = &quot;AIPW&quot;) att &lt;- c(att, att[1] - 1.96 * att[2], att[1] + 1.96 * att[2]) return(att) } aipw.match &lt;- function(data, Y, treat, covar) { # match on ps ps &lt;- probability_forest(X = data[, covar], Y = as.factor(data[, treat]), seed = 1234)$predictions[,2] m.out &lt;- Match(Y = data[, Y], Tr = data[, treat], X = ps, estimand = &quot;ATT&quot;, M = 1, replace = FALSE, ties = FALSE, BiasAdjust = FALSE) mb &lt;- quiet(MatchBalance(treat ~ ps, data = data, match.out = m.out, nboots= 0)) ks &lt;- mb$AfterMatching[[1]]$ks$ks$statistic s &lt;- data[c(m.out$index.treated, m.out$index.control), ] out &lt;- aipw(s, Y, treat, covar) #return(out) return(c(out, ks)) } aipw_ow &lt;- function(data, Y, treat, covar) { for (var in c(Y, treat, covar)) { data[, var] &lt;- as.vector(data[, var]) } X &lt;- data[, covar, drop = FALSE] Y &lt;- data[, Y] W &lt;- data[, treat] # run dml_with_smoother with AIPW_ATT dml_fit &lt;- dml_with_smoother(Y = Y, D = W, X = X, estimators = c(&quot;AIPW_ATT&quot;), smoother = &quot;honest_forest&quot;, n_cf_folds = 5, n_reps = 1) # extract estimate and SE from summary summ &lt;- summary(dml_fit, quiet = TRUE) est &lt;- summ[&quot;AIPW-ATT&quot;, &quot;Estimate&quot;] se &lt;- summ[&quot;AIPW-ATT&quot;, &quot;SE&quot;] ci_lower &lt;- est - 1.96 * se ci_upper &lt;- est + 1.96 * se return(c(est, se, ci_lower, ci_upper)) } ### dml dml &lt;-function(data, Y = NULL, treat = NULL, covar = NULL, clust_var = NULL, ml_l = lrn(&quot;regr.lm&quot;), ml_m = lrn(&quot;regr.lm&quot;)){ if(is.null(covar)){ stop(&quot;No controls in specification.&quot;) } #require(DoubleML) #require(mlr3learners) #require(fixest) #require(ggplot2) if(is.null(clust_var) == TRUE){ dat = data[,c(Y,treat,covar)] dat = na.omit(dat) dml_dat = DoubleMLData$new(dat, y_col = Y, d_cols = treat, use_other_treat_as_covariate = FALSE, x_cols = covar) }else{ dat = data[,c(Y, treat, covar, clust_var)] dat[,clust_var] = as.numeric(factor(dat[,clust_var])) dat = dat[is.na(dat[,Y]) == FALSE,] dat = dat[is.na(dat[,D]) == FALSE,] features = data.frame(model.matrix(formula(paste(c(&#39;~ 1&#39;,treat,covar), collapse=&quot;+&quot;)), dat)) dat = cbind(dat[,c(Y,clust_var)],features) dml_dat = DoubleMLClusterData$new(dat, y_col = Y, d_cols = treat, cluster_cols = clust_var, use_other_treat_as_covariate = FALSE, x_cols = covar) } # Set active treatment treatment dml_dat$set_data_model(treat) # Estimate with DML set.seed(pi) dml_mod = DoubleMLPLR$new(dml_dat, ml_l=ml_l, ml_m=ml_m) quiet(dml_mod$fit()) out = c(dml_mod$coef[treat], dml_mod$se[treat], dml_mod$confint()[treat,]) return(out) } # execute all estimators estimate_all &lt;- function(data, Y, treat, covar, methods = c(&quot;diff&quot;, &quot;reg&quot;, &quot;om.reg&quot;, &quot;om.grf&quot;, &quot;matching&quot;, &quot;psm&quot;, &quot;ipw&quot;, &quot;cbps&quot;, &quot;ebal&quot;, &quot;dml&quot;, &quot;aipw_grf&quot;, &quot;aipw_ow&quot;)) { results &lt;- as.data.frame(matrix(NA, length(methods), 4)) rownames(results) &lt;- methods colnames(results) &lt;- c(&quot;Estimate&quot;, &quot;SE&quot;, &quot;CI_lower&quot;, &quot;CI_upper&quot;) m &lt;- 1 if (&quot;diff&quot; %in% methods) { results[m, ] &lt;- diff(data, Y, treat) m &lt;- m + 1 } if (&quot;reg&quot; %in% methods) { results[m, ] &lt;- reg(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;om.reg&quot; %in% methods) { results[m, ] &lt;- om.reg(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;om.grf&quot; %in% methods) { results[m, ] &lt;- om.grf(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;matching&quot; %in% methods) { results[m, ] &lt;- matching(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;psm&quot; %in% methods) { results[m, ] &lt;- psm(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;ipw&quot; %in% methods) { results[m, ] &lt;- ipw(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;cbps&quot; %in% methods) { results[m, ] &lt;- cbps(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;ebal&quot; %in% methods) { results[m, ] &lt;- quiet(ebal(data, Y, treat, covar)) m &lt;- m + 1 } # if (&quot;hbal&quot; %in% methods) { # results[m, ] &lt;- quiet(hbal(data, Y, treat, covar)) # m &lt;- m + 1 # } if (&quot;dml&quot; %in% methods) { results[m, ] &lt;-dml(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;aipw_grf&quot; %in% methods) { results[m, ] &lt;- aipw(data, Y, treat, covar) m &lt;- m + 1 } if (&quot;aipw_ow&quot; %in% methods) { results[m, ] &lt;- aipw_ow(data, Y, treat, covar) m &lt;- m + 1 } return(results) } Function calls quiet: Suppresses output from a function call. diff: Difference in means estimator. It runs a linear regression adjusting for robust standard errors and returns the coefficient, standard error, and confidence interval for the treatment variable. reg: Regression adjustment. Similar to diff but includes additional covariates in the regression model. matching: Propensity score matching using the Matching package. It aligns treated units to control units based on covariates and returns the estimated ATT and its confidence interval. psm: Propensity score matching using a probability forest, followed by matching and estimation of the ATT. om.reg: Outcome modeling using regression. It predicts the outcome for the treated units based on the model fitted to the control units, and then estimates the ATT. om.grf: Outcome modeling using generalized random forests, noted as GRF. ipw: Inverse probability weighting,denoted as IPW. It weights observations by the inverse of their estimated propensity scores and calculates the treatment effect with a weighted regression. cbps: Covariate balancing propensity score, noted as CBPS. It estimates propensity scores to achieve balance on covariates across groups. ebal: Entropy balancing. It reweights the data to balance the covariate distributions. hbal: Hierarchical balancing. It is an extension of ebal with more complex balancing methods. aipw: Augmented inverse probability weighting, noted as AIPW. aipw.match: Combines matching on propensity scores with AIPW. aipw_ow: AIPW computed via outcome weights. dml: Double machine learning. It uses machine learning algorithms to control for confounders when estimating treatment effects. plot_coef() plots treatment effect point estimates and corresponding confidence intervals comparatively across various estimators. plot_coef &lt;- function(out, methods = c(&quot;diff&quot;, &quot;reg&quot;, &quot;om.reg&quot;, &quot;om.grf&quot;, &quot;matching&quot;, &quot;psm&quot;, &quot;ipw&quot;, &quot;cbps&quot;, &quot;ebal&quot;, &quot;dml&quot;, &quot;aipw_grf&quot;, &quot;aipw_ow&quot;), labels = c(&quot;Diff-in-Means&quot;, &quot;Reg&quot;, &quot;OM: Reg&quot;, &quot;OM: GRF&quot;, &quot;NN\\nMatching&quot;, &quot;PS\\nMatching&quot;, &quot;IPW&quot;, &quot;CBPS&quot;, &quot;Ebal&quot;, &quot;DML\\nElasnet&quot;, &quot;AIPW-GRF&quot;, &quot;AIPW-OW&quot;), main = NULL, ylab = &quot;Estimate&quot;, band = NULL, line = NULL, grid = TRUE, main.pos = 1, main.line = -2, ylim = NULL, textsize = 1 ) { if (is.null(methods) == TRUE) { methods &lt;- rownames(out) } if (is.null(labels) == TRUE) { labels &lt;- methods } # # check # if (is.null(out)==FALSE) { # if (inherits(out, &quot;ivDiag&quot;) == FALSE) {stop(&quot;\\&quot;out\\&quot; needs to be a \\&quot;ltz\\&quot; object.&quot;)} # } # # # title # if (is.null(main)==TRUE) { # main &lt;- &quot;Estimates with 95% CIs&quot; # } # data for the plot data &lt;- out rg &lt;- range(data[,c(3,4)], na.rm = TRUE) adj &lt;- rg[2] - rg[1] if (is.null(ylim) == TRUE) { ylim &lt;- c(min(0, rg[1] - 0.3*adj), max(0, rg[2] + 0.35*adj)) } adj2 &lt;- ylim[2] - ylim[1] # Set up the plot ncoefs &lt;- length(methods) par(mar = c(2.5, 4, 1, 2)) plot(1: ncoefs, data[, 1], xlim = c(0.5, ncoefs + 0.5), ylim = ylim, ylab = &quot;&quot;, xlab = &quot;&quot;, main = &quot;&quot;, axes = FALSE, xaxt = &quot;n&quot;, yaxt = &quot;n&quot;, type = &quot;n&quot;) axis(1, at = 1: ncoefs, labels = labels, las = 1, cex.axis = 0.8) axis(2, cex.axis = 0.7) mtext(main, main.pos, line = main.line, cex = textsize) mtext(ylab, 2, line = 2.5) if (is.null(band) == FALSE) { rect(-0.5, band[1], ncoefs + 1, band[2], col = &quot;#ff000030&quot;, border = &quot;white&quot;) # label at bottom } if (is.null(line) == FALSE) { abline(h = line, col = &quot;red&quot;, lty = 2) } if (grid == TRUE) { abline(h = axTicks(2), lty = &quot;dotted&quot;, col = &quot;gray50&quot;) abline(v = c(0.5, c(1: ncoefs) + 0.5), lty = &quot;dotted&quot;, col = &quot;gray50&quot;) # horizontal grid } abline(h = 0, col = &quot;red&quot;, lwd = 2, lty = &quot;solid&quot;) segments(y0 = data[, 3], x0 = c(1: ncoefs), y1 = data[, 4], x1 = c(1: ncoefs), lwd = 2) #CI points(1: ncoefs, data[, 1], pch = 16, col = 1, cex = 1.2) #point coefs box() } plot_att_panels() produces panels of ATT estimates with confidence intervals for multiple methods and datasets. plot_att_panels &lt;- function(all_outs, plot_titles, band, est, ylim = c(-15500, 5500), plots_per_page = 4, ylab = &quot;Estimate&quot;, textsize = 1) { num_pages &lt;- ceiling(length(all_outs) / plots_per_page) for (page in seq_len(num_pages)) { start_idx &lt;- (page - 1) * plots_per_page + 1 end_idx &lt;- min(page * plots_per_page, length(all_outs)) par(mfrow = c(plots_per_page, 1), mar = c(3, 4, 3, 2)) for (i in start_idx:end_idx) { out &lt;- all_outs[[i]] plot_coef(out, band = band, line = est, ylim = ylim, main = plot_titles[i], ylab = ylab, textsize = textsize) } } } Arguments all_outs: A list of output objects containing ATT estimates and confidence intervals for different methods. plot_titles: A character vector of titles for each plot, corresponding to the outputs in all_outs. band: A numeric vector specifying the confidence interval bounds. est: A numeric value representing the point estimate. ylim: A numeric vector specifying the y-axis limits for the plots (default c(-15500, 5500)). plots_per_page: The number of plots to display per page / panel (default 4). ylab: The label for the y-axis (default “Estimate”). textsize: A numeric scaling factor for text size in plots (default 1). save_att_panels() generates and saves paginated pdf plots that display ATT estimates and their confidence bands from multiple methods. save_att_panels &lt;- function( all_outs, plot_titles, band, est, prefix, plots_per_page = 4, ylab = &quot;Estimate&quot;, textsize = 1) { folder &lt;- &quot;../graphs/lalonde&quot; ylim &lt;- c(-15500, 5500) if (!dir.exists(folder)) dir.create(folder, recursive = TRUE) num_pages &lt;- ceiling(length(all_outs) / plots_per_page) for (page in seq_len(num_pages)) { file_name &lt;- file.path(folder, paste0(prefix, &quot;_&quot;, page, &quot;.pdf&quot;)) pdf(file_name, width = 8, height = 11) par(mfrow = c(plots_per_page, 1), mar = c(3,4,3,2)) start_idx &lt;- (page - 1) * plots_per_page + 1 end_idx &lt;- min(page * plots_per_page, length(all_outs)) for (i in start_idx:end_idx) { plot_coef(all_outs[[i]], band = band, line = est, ylim = ylim, main = plot_titles[i], ylab = ylab, textsize = textsize) } dev.off() } } Arguments all_outs: A list of output objects containing ATT estimates and confidence intervals for different methods. plot_titles: A character vector of titles for each plot, corresponding to the outputs in all_outs. band: A numeric vector specifying the confidence interval bounds. est: A numeric value representing the point estimate. prefix: String prefix used for saved file names to identify dataset. plots_per_page: The number of plots to display per page / panel (default 4). ylab: The label for the y-axis (default “Estimate”). textsize: A numeric scaling factor for text size in plots (default 1). create_matrix_results() organizes estimation results into a formatted matrix suitable for reporting or tables. create_matrix_results &lt;- function(all_outs, sample_names) { n_samples &lt;- length(sample_names) n_estimators &lt;- nrow(all_outs[[1]]) result_mat &lt;- matrix(&quot;&quot;, nrow = n_estimators + 1, ncol = n_samples * 2) # Set up alternating column names cnames &lt;- character(n_samples * 2) for (j in seq_along(sample_names)) { cnames[(j-1)*2 + 1] &lt;- sample_names[j] cnames[(j-1)*2 + 2] &lt;- &quot;&quot; # SE/parenthesis column } colnames(result_mat) &lt;- cnames estimator_names &lt;- rownames(all_outs[[1]]) rownames(result_mat) &lt;- c(&quot;Experimental Benchmark&quot;, estimator_names) # Fill values for (j in seq_along(all_outs)) { out &lt;- all_outs[[j]] result_mat[1, (j-1)*2 + 1] &lt;- sprintf(&quot;%.2f&quot;, out[1, 1]) result_mat[1, (j-1)*2 + 2] &lt;- paste0(&quot;(&quot;, sprintf(&quot;%.2f&quot;, out[1, 2]), &quot;)&quot;) for (i in 2:(n_estimators+1)) { result_mat[i, (j-1)*2 + 1] &lt;- sprintf(&quot;%.2f&quot;, out[i-1, 1]) result_mat[i, (j-1)*2 + 2] &lt;- paste0(&quot;(&quot;, sprintf(&quot;%.2f&quot;, out[i-1, 2]), &quot;)&quot;) } } return(result_mat) } Arguments all_outs: A list of output objects containing ATT estimates and confidence intervals for different methods. sample_names: A character vector of sample / dataset names corresponding to all_outs. eval_att() computes summary statistics of the treatment effect estimates. eval_att &lt;- function(result) { data.frame( Mean_SE = mean(result[, &quot;SE&quot;], na.rm = TRUE), # mean standard error across all estimator results Min_Estimate = min(result[, &quot;Estimate&quot;], na.rm = TRUE), # maximum estimate of all estimator results Max_Estimate = max(result[, &quot;Estimate&quot;], na.rm = TRUE), # minimum estimate of all estimator results Diff_Estimate = max(result[, &quot;Estimate&quot;], na.rm = TRUE) - min(result[, &quot;Estimate&quot;], na.rm = TRUE) ) } Argument result: A data frame containing treatment effect estimation results. 1.2.5.2 Conditional average treatment effect on the treated (CATT) plot_catt_panels() visualizes CATT comparing multiple methods. plot_catt_panels &lt;- function(all_catt, plot_titles, plots_per_page = 4, range = c(-8000, 8000)) { num_pages &lt;- ceiling((length(all_catt) - 1) / plots_per_page) # Experimental reference (first panel) catt_ldw &lt;- all_catt[[1]]$catt att_ldw &lt;- all_catt[[1]]$att[1] id_ldw &lt;- if (!is.null(all_catt[[1]]$id)) all_catt[[1]]$id else seq_along(catt_ldw) for (page in seq_len(num_pages)) { start_idx &lt;- (page - 1) * plots_per_page + 2 # skip experimental vs itself end_idx &lt;- min(page * plots_per_page + 1, length(all_catt)) par(mfrow = c(plots_per_page, 1), mar = c(4.5, 5, 3, 2)) for (i in start_idx:end_idx) { other &lt;- all_catt[[i]] main_label &lt;- plot_titles[i] catt2 &lt;- other$catt att2 &lt;- other$att[1] id2 &lt;- if (!is.null(other$id)) other$id else seq_along(catt2) common_ids &lt;- intersect(id_ldw, id2) idx_ldw &lt;- match(common_ids, id_ldw) idx_other &lt;- match(common_ids, id2) catt1_plot &lt;- catt_ldw[idx_ldw] catt2_plot &lt;- catt2[idx_other] plot_catt( catt1 = catt1_plot, catt2 = catt2_plot, att1 = att_ldw, att2 = att2, xlab = &quot;CATT (Experimental)&quot;, ylab = main_label, main = main_label, axes.range = range ) } } } Arguments all_catt: A list of objects containing conditional average treatment effect on treated (CATT) estimates. plot_titles: A character vector of titles for each plot, corresponding to the outputs in all_catt. plots_per_page: The number of plots to display per page / panel (default 4). range: A numeric vector of axis limits for plots (default c(-8000, 8000)). save_catt_panels()generates and saves multi-page PDF plots comparing CATT estimates across different methods. save_catt_panels &lt;- function(all_catt, plot_titles, range = c(-8000, 8000), prefix = &quot;model_a&quot;, plots_per_page = 4) { dir.create(&quot;../graphs/lalonde&quot;, showWarnings = FALSE, recursive = TRUE) catt_ldw &lt;- all_catt[[1]]$catt att_ldw &lt;- all_catt[[1]]$att[1] id_ldw &lt;- if (!is.null(all_catt[[1]]$id)) all_catt[[1]]$id else seq_along(catt_ldw) num_panels &lt;- length(all_catt) - 1 # skip experimental on page num_pages &lt;- ceiling(num_panels / plots_per_page) for (page in seq_len(num_pages)) { start_idx &lt;- (page - 1) * plots_per_page + 2 # always skip all_catt[[1]] end_idx &lt;- min(page * plots_per_page + 1, length(all_catt)) plots_this_page &lt;- end_idx - start_idx + 1 file_name &lt;- sprintf(&quot;../graphs/lalonde/%s_catt_estimates_%d.pdf&quot;, prefix, page) pdf(file = file_name, width = 10, height = 12) par(mfrow = c(2, 2), mar = c(4, 4, 2, 2)) for (i in start_idx:end_idx) { other &lt;- all_catt[[i]] main_label &lt;- plot_titles[i] catt2 &lt;- other$catt att2 &lt;- other$att[1] id2 &lt;- if (!is.null(other$id)) other$id else seq_along(catt2) common_ids &lt;- intersect(id_ldw, id2) idx_ldw &lt;- match(common_ids, id_ldw) idx_other &lt;- match(common_ids, id2) catt1_plot &lt;- catt_ldw[idx_ldw] catt2_plot &lt;- catt2[idx_other] plot_catt( catt1 = catt1_plot, catt2 = catt2_plot, att1 = att_ldw, att2 = att2, xlab = &quot;CATT (Experimental)&quot;, ylab = main_label, main = main_label, axes.range = range ) } # blanks if fewer than 4 plots on last page if (plots_this_page &lt; plots_per_page) { for (k in seq_len(plots_per_page - plots_this_page)) plot.new() } dev.off() } } Arguments all_catt: A list of objects containing conditional average treatment effect on treated (CATT) estimates. plot_titles: A character vector of titles for each plot, corresponding to the outputs in all_catt. range: A numeric vector of axis limits for plots (default c(-8000, 8000)). prefix: A string prefix for saved PDF filenames. plots_per_page: The number of plots to display per page / panel (default 4). eval_catt() computes summary statistics of CATT vectors for each method, returning the minimum, maximum, mean, and range (difference) of the CATT estimates. eval_catt &lt;- function(all_catt, plot_titles) { do.call(rbind, lapply(seq_along(all_catt), function(i) { catt_vec &lt;- all_catt[[i]]$catt data.frame( Method = plot_titles[i], Min_Catt = min(catt_vec, na.rm = TRUE), Max_Catt = max(catt_vec, na.rm = TRUE), Mean_Catt = mean(catt_vec, na.rm = TRUE), Diff_Catt = max(catt_vec, na.rm = TRUE) - min(catt_vec, na.rm = TRUE), stringsAsFactors = FALSE ) })) } Arguments all_catt: A list of objects containing conditional average treatment effect on treated (CATT) estimates. plot_titles: A character vector of titles for each plot, corresponding to the outputs in all_catt. 1.2.5.3 Quantile treatment effect on treated (QTET) plot_qte_top() plots QTET of top-ranked samples for comparison against an experimental benchmark. plot_qte_top &lt;- function(qtet_top, qtet_top0, bm, plot_titles, main_start = 1, ylim = NULL, col = NULL) { n &lt;- length(qtet_top) for (i in 1:n) { main_title &lt;- plot_titles[main_start + i - 1] mod &lt;- qtet_top[[i]] mod2 &lt;- qtet_top0[[i]] plot_qte(mod, mod2, bm, main = main_title, ylim = ylim, col = col) legend(&quot;bottomleft&quot;, legend = c(&quot;Experimental&quot;, &quot;Unadjusted&quot;, &quot;Adjusted&quot;), lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = &quot;n&quot;) } } Arguments qtet_top: A list of objects containing QTET estimates for top-ranked samples. qtet_top0: A list of objects containing unadjusted QTET estimates. bm: Experimental benchmark. plot_titles: A character vector of titles for each plot, matching the order of elements in qtet_top. main_start: Integer index specifying where to start selecting titles from plot_titles (default is 1). ylim: A numeric vector specifying the y-axis limits for the plots. col: Color specification. save_qtet() generates and saves quantile treatment effect on the treated (QTET) plots for a given list of estimates. save_qtet &lt;- function(plots, plot_titles = NULL, main_start = 1, ylim = NULL, col = NULL, prefix = &quot;model_a&quot;) { dir.create(&quot;../graphs/lalonde&quot;, showWarnings = FALSE, recursive = TRUE) n &lt;- length(plots) for (i in seq_len(n)) { p &lt;- plots[[i]] main_title &lt;- if (is.null(plot_titles)) p$main else plot_titles[main_start + i - 1] clean_title &lt;- gsub(&quot;[^a-zA-Z0-9]&quot;, &quot;_&quot;, main_title) file_name &lt;- sprintf(&quot;../graphs/lalonde/%s_qtet_estimates_%s.pdf&quot;, prefix, clean_title) pdf(file = file_name, width = 7, height = 5) plot_qte(p$mod, p$mod0, p$bm, main = main_title, ylim = ylim, col = col) legend(&quot;bottomleft&quot;, legend = c(&quot;Experimental&quot;, &quot;Unadjusted&quot;, &quot;Adjusted&quot;), lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = &quot;n&quot;) dev.off() } } Arguments plots: A list of objects containing QTET estimates and corresponding unadjusted estimates. plot_titles: A character vector of titles for each plot, matching the order of elements in plots. main_start: Integer index specifying where to start selecting titles from plot_titles (default is 1). ylim: A numeric vector specifying the y-axis limits for the plots. col: Color specification. prefix: A string prefix for saved PDF filenames. save_qte_top() generates and saves quantile treatment effect on the treated (QTET) plots for a given list of estimates of top-ranked samples. save_qte_top &lt;- function(qtet_top, qtet_top0, bm, plot_titles, main_start = 1, ylim = NULL, col = NULL, prefix = &quot;model_a_top&quot;) { n &lt;- length(qtet_top) dir.create(&quot;../graphs/lalonde&quot;, showWarnings = FALSE, recursive = TRUE) for (i in seq_len(n)) { mod &lt;- qtet_top[[i]] mod2 &lt;- qtet_top0[[i]] main_title &lt;- plot_titles[main_start + i - 1] clean_title &lt;- gsub(&quot;[^a-zA-Z0-9]&quot;, &quot;_&quot;, main_title) file_name &lt;- sprintf(&quot;../graphs/lalonde/%s_qte_estimates_%s.pdf&quot;, prefix, clean_title) pdf(file = file_name, width = 7, height = 5) plot_qte(mod, mod2, bm, main = main_title, ylim = ylim, col = col) legend(&quot;bottomleft&quot;, legend = c(&quot;Experimental&quot;, &quot;Unadjusted&quot;, &quot;Adjusted&quot;), lty = 1, pch = c(16, 17, 16), col = c(4, 2, 1), bty = &quot;n&quot;) dev.off() } } Arguments qtet_top: A list of objects containing QTET estimates for top-ranked samples. qtet_top0: A list of objects containing unadjusted QTET estimates. plot_titles: A character vector of titles for each plot, matching the order of elements in qtet_top. main_start: Integer index specifying where to start selecting titles from plot_titles (default is 1). ylim: A numeric vector specifying the y-axis limits for the plots. col: Color specification. prefix: A string prefix for saved PDF filenames. 1.3 Outcome weights analysis get_res_att() estimates average treatment effects on the treated (ATT) using augmented inverse probability weighting with cross-fitting on a list of datasets. get_res_att &lt;- function(dataset_list, Y, treat, covar, estimators = &quot;AIPW_ATT&quot;, smoother = &quot;honest_forest&quot;, n_cf_folds = 5, n_reps = 1) { lapply(dataset_list, function(data) { dml_with_smoother( Y = data[[Y]], D = data[[treat]], X = data[, covar, drop = FALSE], estimators = estimators, smoother = smoother, n_cf_folds = n_cf_folds, n_reps = n_reps ) } ) } Arguments dataset_list: A list of input datasets. Y: String specifying the name of the outcome variable. treat: String specifying the name of the binary treatment indicator variable. covar: A vector of covariate names. estimators: String specifying the estimator to use (default is “AIPW_ATT”). smoother: String specifying the smoothing method (default is “honest_forest”). n_cf_folds: Integer specifying the number of cross-fitting folds (default is 5). n_reps: Integer specifying the number of repetitions for robustness (default is 1). derive_ow() extracts outcome weights from a list of estimation results. derive_ow &lt;- function(results_list) { lapply(results_list, function(res) get_outcome_weights(res)) } Argument results_list: A list of objects obtained from get_res_att(). plot_ow() plot_ow &lt;- function(outcome_weights, plot_titles = NULL, breaks = 50, col = &quot;#ff000080&quot;, xlab = &quot;Outcome Weight&quot;, estimator = &quot;AIPW-ATT&quot;) { N &lt;- length(outcome_weights) for (i in seq_len(N)) { weights &lt;- outcome_weights[[i]]$omega[estimator, ] main_title &lt;- if (!is.null(plot_titles)) plot_titles[i] else paste(&quot;Dataset&quot;, i) hist(weights, breaks = breaks, main = main_title, xlab = xlab, col = col) mtext(paste(&quot;N =&quot;, length(weights)), side = 3, line = -1.5, cex = 0.8) } par(mfrow = c(1, 1)) } Arguments outcome_weights: A list of outcome weight objects. plot_titles: A character vector of titles for each plot. breaks: Integer specifying the number of bins for the histogram plots (default is 50). col: Color specification (default is “#ff000080”). xlab: String specifying the label for the x-axis (default is “Outcome Weight”) estimator: String specifying the estimator to use (default is “AIPW-ATT”). eval_ow() computes summary statistics of outcome weights by treatment group for a list of datasets and returns a data frame for comparison. eval_ow &lt;- function(outcome_weights, dataset_list, plot_titles = NULL, treat_var = &quot;treat&quot;, estimand = &quot;AIPW-ATT&quot;) { results &lt;- lapply(seq_along(outcome_weights), function(i) { ow &lt;- outcome_weights[[i]]$omega[estimand, ] treat &lt;- dataset_list[[i]][[treat_var]] method &lt;- if (!is.null(plot_titles)) plot_titles[i] else paste(&quot;Dataset&quot;, i) sum_treated &lt;- sum(ow[treat == 1]) sum_untreated &lt;- sum(ow[treat == 0]) data.frame( Method = method, Sum_Treated = sum_treated, Sum_Untreated = sum_untreated, stringsAsFactors = FALSE ) }) do.call(rbind, results) } **Arguments* outcome_weights: A list of outcome weight objects. plot_titles: A character vector of titles for each plot. treat_var: String specifying the treatment variable in each dataset (default is “treat”). estimator: String specifying the estimator to use (default is “AIPW-ATT”). save_ow() saves the outcome weight histograms as PDF files, one per dataset, with optional custom titles and formatting. save_ow &lt;- function(outcome_weights, plot_titles = NULL, breaks = 50, col = &quot;#ff000080&quot;, xlab = &quot;Outcome Weight&quot;, prefix = &quot;model_a&quot;, estimator = &quot;AIPW-ATT&quot;) { dir.create(&quot;../graphs/lalonde&quot;, showWarnings = FALSE, recursive = TRUE) N &lt;- length(outcome_weights) for (i in seq_len(N)) { file_name &lt;- sprintf(&quot;../graphs/lalonde/%s_outcomewt_%d.pdf&quot;, prefix, i) pdf(file = file_name, width = 8, height = 6) weights &lt;- outcome_weights[[i]]$omega[estimator, ] main_title &lt;- if (!is.null(plot_titles)) plot_titles[i] else paste(&quot;Dataset&quot;, i) hist(weights, breaks = breaks, main = main_title, xlab = xlab, col = col) mtext(paste(&quot;N =&quot;, length(weights)), side = 3, line = -1.5, cex = 0.8) dev.off() } } Argument outcome_weights: A list of outcome weight objects. plot_titles: A character vector of titles for each plot. breaks: Integer specifying the number of bins for the histogram plots (default is 50). col: Color specification (default is “#ff000080”). xlab: String specifying the label for the x-axis (default is “Outcome Weight”) prefix: A string prefix for saved PDF filenames. estimator: String specifying the estimator to use (default is “AIPW-ATT”). 1.4 Sensitivity analysis check_filter_datasets() filters a list of datasets to retain only those with all required variables, no missing data, binary treatment, and sufficient baseline/covariate variation. check_filter_datasets &lt;- function(datasets, Y, treat, covar, bm) { valid_datasets &lt;- list() for (i in seq_along(datasets)) { data &lt;- datasets[[i]] name &lt;- names(datasets)[i] if (is.null(name) || name == &quot;&quot;) name &lt;- paste0(&quot;dataset_&quot;, i) # provide a name if missing vars_needed &lt;- c(Y, treat, covar, bm) if (!all(vars_needed %in% names(data))) { # check all variables exist message(&quot;Removed &quot;, name, &quot;: missing required variables&quot;) next } sub &lt;- data[, vars_needed, drop = FALSE] if (any(is.na(sub))) { # check no missing values message(&quot;Removed &quot;, name, &quot;: contains missing values&quot;) next } tvals &lt;- unique(sub[[treat]]) # check treatment binary if (!all(tvals %in% c(0, 1)) &amp;&amp; !all(tvals %in% c(TRUE, FALSE))) { message(&quot;Removed &quot;, name, &quot;: treatment variable not binary&quot;) next } if (any(sapply(sub[bm], function(x) length(unique(x)) &lt;= 1))) { # check baseline variables have variation message(&quot;Removed &quot;, name, &quot;: baseline variable(s) lack variation&quot;) next } if (any(sapply(sub[covar], function(x) length(unique(x)) &lt;= 1))) { # check covariates have variation message(&quot;Removed &quot;, name, &quot;: covariate variable(s) lack variation&quot;) next } valid_datasets[[name]] &lt;- data # if all passed dataset is kept } return(valid_datasets) } Arguments datasets: The input datasets. Y: The name of the outcome variable. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. bm: A vector of baseline variable names. save_sensitivity_plots() generates sensitivity analysis plots for a series of filtered datasets, saving each as a pdf file. save_sensitivity_plots &lt;- function(filtered_datasets, Y, treat, covar, bm, plot_titles, prefix) { folder &lt;- &quot;../graphs/lalonde&quot; if (!dir.exists(folder)) dir.create(folder, recursive = TRUE) for (i in seq_along(filtered_datasets)) { idx &lt;- i file_name &lt;- file.path(folder, paste0(prefix, &quot;_sensitivity_&quot;, idx, &quot;.pdf&quot;)) pdf(file_name, width = 8, height = 8) sens_ana(filtered_datasets[[i]], Y, treat, covar, bm, kd = 1:3) if (!missing(plot_titles) &amp;&amp; length(plot_titles) &gt;= idx) { title(main = plot_titles[idx]) } dev.off() } } Arguments filtered_datasets: A list of filtered datasets. Y: The name of the outcome variable. treat: The name of the binary treatment indicator variable. covar: A vector of covariate names. bm: A vector of baseline variable names. plot_titles: A vector of titles for plots corresponding to datasets. prefix: String prefix for saving output PDF files. Please refer to the accompanying paper for a detailed explanation of the metrics used in the analysis, including absolute standardized mean differences (SMDs), effective sample sizes (ESS), and the composite scoring approach employed to evaluate covariate balance and overlap across multiple methods. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
